---
layout: post
title:  "Machine Learning - from coursera"
date:   2017-08-12
categories: codeLearning
---

# What is mechine learning

"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E." --Tom Mitchell

---
## Types

1. Supervised learning

In supervised learning, we are given a data set and already know what our correct output should look like.

- recognition

predict results within a continuous output

- classification

predict results in a discrete output

2. Unsupervised learning

- clustering

Find structures in the data

- Non-clustering

Cocktail party algorithm

---
## Model

A pair (x(i),y(i)) is called a training example, a list of m training examples (x(i),y(i));i=1,...,m—is called a training set.

Our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. This function h is called a hypothesis.

<img src='/images/hypothesis.png'>

---
## Cost function

`cost function` is used to measure the accuracy of the hypothesis function.

Our goal is t minimize the value of cost function.

<img src='/images/costFunction.png'>

<img src='/images/cost_function.png'>

## Gradient descent

The gradient descent algorithm：

<img src='/images/gradientDescentAlgorithm.png'>

:= assignment
a: learning rate

simulteneously update: θ(0), θ(1) for example

<img src='/images/simulteneouslyUpdate.png'>

---
## Linear regression with multiple features

<img src='/images/multipleFeature1.png'>

<img src='/images/multipleFeature2.png'>

Greadient descent for multiple features:

<img src='/images/gradientDescentMulti.png'>

---
## Practical Tips 

- Feature scalling

make sure each feature in −1 ≤ x(i) ≤ 1, by xi:=(xi−μi)/si

- Learning rate

Make convergence test of cost function to adjust learningrate(a):

<img src='/images/learningRate1.png'>

<img src='/images/learningRate2.png'>

- polynominal regression

We can combine multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking x1⋅x2.

We can change the **behavior or curve** of our hypothesis function by making it a quadratic, cubic or square root function

	quadratic function: hθ(x)=θ0+θ1*x1+θ2*x1^2
	cubic function: hθ(x)=θ0+θ1*x1+θ2*x1^2+θ3x1^3

In the cubic version, we have created new features x2 and x3 where `x2=x1^2 and x3=x1^3`.

Then feature scaling becomes very important now. make sure −1 ≤ x(i) ≤ 1.

---
## Normal Equation
The analytical solution for θ.

<img src='/images/normalEquation.png'>

<img src='/images/normalEquation1.png'>

Comparison of gradient descent and the normal equation:

<img src='/images/normalEquation2.png'>

- Normal equation noninvertable

When implementing the normal equation in octave we want to use the 'pinv' function rather than 'inv.' The 'pinv' function will give you a value of θ even if XT*X is not invertible.

If XT*X is noninvertible, the common causes might be having :

> - Redundant features, where two features are very closely related (i.e. they are linearly dependent)
> - Too many features (e.g. m ≤ n). In this case, delete some features or use "regularization" (to be explained in a later lesson).

Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.

---
##Classification

































